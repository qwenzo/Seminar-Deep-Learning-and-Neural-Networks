\relax 
\citation{hassoun1995fundamentals}
\citation{frankpic}
\citation{amari1982competition}
\citation{hochreiter1997long}
\citation{lecun1998gradient}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\@writefile{toc}{\contentsline {title}{Neural Networks and Deep Learning }{1}\protected@file@percent }
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Mohamed Hesham Ibrahim Abdalla \and Slim Abdennadher \and Alia Elbolock}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\newlabel{fpp}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Frank Rosenblatt with a Mark I Perceptron computer \cite  {frankpic}.}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Background and Concept}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Forward Propagation}{2}\protected@file@percent }
\newlabel{fb}{{2.1}{2}}
\newlabel{nn}{{2.1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) A single-layer \gls {ann} where the first layer has only one neuron. (b) A multi-layer \gls {ann} with 2 hidden layers. }}{2}\protected@file@percent }
\citation{nonsep}
\newlabel{z}{{1}{3}}
\newlabel{a}{{2}{3}}
\newlabel{zeq1}{{3}{3}}
\newlabel{zeq2}{{4}{3}}
\newlabel{b}{{5}{3}}
\citation{lemarechal2012cauchy}
\citation{gdwiki}
\citation{gdwiki}
\newlabel{nls}{{2.1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of a linear activation function on non-linearly separable data \cite  {nonsep}. }}{4}\protected@file@percent }
\newlabel{log}{{6}{4}}
\newlabel{maeq}{{7}{4}}
\newlabel{mseq}{{8}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Back Propagation}{4}\protected@file@percent }
\citation{sdc}
\citation{sdc}
\newlabel{gd}{{2.2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Gradient descent visualization where the function is moving from a local maxima to a local minima \cite  {gdwiki}.}}{5}\protected@file@percent }
\newlabel{gdu}{{9}{5}}
\newlabel{gdu}{{10}{5}}
\citation{hoffmann1992computer}
\citation{lecun1998gradient}
\newlabel{actfig}{{2.2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A figure of some activation functions where the first one is sigmoid, the second is Tanh and the last is ReLU \cite  {sdc}.}}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Applications and Architectures}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convolutional Neural Networks}{6}\protected@file@percent }
\newlabel{fm}{{3.1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Feature map of size 3x3 applied on a 5x5 binary image https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2}}{7}\protected@file@percent }
\newlabel{mp}{{3.1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces a 2x2 Max Pooling operator on a 4x4 gray scale image. htttps://cs231n.github.io/convolutional-networks/}}{7}\protected@file@percent }
\citation{mckinney2020international}
\newlabel{dcnnp}{{3.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A CNN applied on a boat image to detect if it falls in dog, cat, boat or bird categories. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/}}{8}\protected@file@percent }
\newlabel{can}{{3.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A CNN applied on a boat image to detect if it falls in dog, cat, boat or bird categories.}}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Recurrent Neural Networks}{8}\protected@file@percent }
\citation{werbos1990backpropagation}
\citation{jagannatha2016structured}
\citation{gu2020lstm}
\citation{earthnet}
\citation{hou2017deep}
\citation{yan2016attribute2image}
\citation{berner2019dota}
\citation{openairubikcube}
\newlabel{rnn}{{11}{9}}
\newlabel{rnny}{{12}{9}}
\newlabel{rnnf}{{3.2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces An unfolded RNN where $W_{hy}$ is the weight vector between the state and the output , $W_{xh}$ is the weight vector between the current input $x_{t}$ and the state $h_{t}$.}}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{9}\protected@file@percent }
\bibstyle{splncs04}
\bibdata{refs}
\bibcite{earthnet}{1}
\bibcite{frankpic}{2}
\bibcite{sdc}{3}
\bibcite{gdwiki}{4}
\bibcite{nonsep}{5}
\bibcite{openairubikcube}{6}
\bibcite{amari1982competition}{7}
\bibcite{berner2019dota}{8}
\bibcite{gu2020lstm}{9}
\bibcite{hassoun1995fundamentals}{10}
\bibcite{hochreiter1997long}{11}
\bibcite{hoffmann1992computer}{12}
\bibcite{hou2017deep}{13}
\bibcite{jagannatha2016structured}{14}
\bibcite{lecun1998gradient}{15}
\bibcite{lemarechal2012cauchy}{16}
\bibcite{mckinney2020international}{17}
\bibcite{werbos1990backpropagation}{18}
\bibcite{yan2016attribute2image}{19}
