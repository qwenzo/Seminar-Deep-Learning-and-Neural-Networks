% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage[acronym]{glossaries}
\usepackage{mathtools}
\usepackage{amssymb}
\graphicspath{ {./images/} }
\newacronym{ann}{ANN}{Artificial Neural Network}
\newacronym{anns}{ANNs}{Artificial Neural Networks}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Neural Networks and Deep Learning \thanks{Supported by organization x.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Mohamed Hesham Ibrahim Abdalla\inst{1}\orcidID{0000-1111-2222-3333} \and
Second Author\inst{1}\orcidID{1111-2222-3333-4444} \and
Third Author\inst{1}\orcidID{2222--3333-4444-5555}}
%
\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{German University in Cairo, Cairo, Egypt}
%  \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The abstract should briefly summarize the contents of the paper in
15--250 words.

\keywords{Neural Networks  \and Deep Learning \and Machine Learning}
\end{abstract}
%
%
%
\section{Introduction}

\gls{anns} and preceptrons are intelligent units that has taken inspiration
from biology, especially the brain (cite). \gls{anns} work by taking labeled inputs
and then trying to find a mathematical rule or function to systematically answer the question
of which label belongs to which input, and later identify labels of new inputs that have never been seen before by the network. For example, inputs could be human images and the labels are 
the gender of the human in that particular image.

The history of \gls{anns} and preceptrons goes back to the 50's and the 60's when 
the first known preceptron was created. The first preceptron was simulated on an IBM 704 computer at Cornell Aeronautical Laboratory in 1957 (cite).  It works by giving(cont)




\section{Mathematical Background and Concept}
\gls{anns} learn how to accomplish a task by following two main steps:
forward propagation and backward propagation. Forward propagation is the process of
predicting labels and computing how deviated these labels from the ones provided in the input data.
On the other hand, backward propagation tries to correct the predictions by minimizing
the diffrence between the input labels and the predicted labels. 
\subsection{Forward Propagation}
\gls{anns} consist of neurons which are connected via links. Each neuron gets an input and produces an output. 
Each input is given a certain weight which makes that specific input has more or less priority in controlling the output of the neuron.
Neurons calculate their output by multiplying their inputs by their weights and applying a bias to the multiplication. Eqaution \ref{z}
shows a linear mapping of a single input.
\begin{equation}
\label{z}
    z^i = w^T * x^i + b
\end{equation}
where $z^i$ is the linear mapping of the $i$th example,  $w^T$ in <- $ R^{1xN} $ is the weight vector of the form $[w_1, w_2, ... w_N]$ and
$x^i$ in <- $ R^{Nx1} $ is the input vector  of the form $[x_1, x_2, ... x_N]$ of the $i$th example.

This mapping is then forwaded to an activation function (Eqaution \ref{a}).
Action functions are used to limit the linear transformation output.
\begin{equation}
    \label{a}
        a^i = g(z^i)
\end{equation}
where $a^i$ is the output of the activation function (unit activation) on the linear mapping of the $i$th example.

The choice of the activation function vary depending on the given data (inputs). 
In general, 
there are three main categories of the activation functions: binary, linear and non-linear. 
Binary or threshold functions output a binary value depending on the input. 
For example, the step function produces a +1 in case $z^i$ is greater than or eqaul to 0 and -1 otherwise (Eqaution \ref{b}).
Binary functions can not deal with categorical data, therefore they are not widely used.
\begin{equation}
    \label{b}
f(x) = \left\{ \begin{array}{ll} +1 \quad z^i \leq 0 \\ -1 \quad \text{otherwise} \end{array} \right.
\end{equation}

Another type of activation functions is linear. 
Linear functions forward the input directly to the output without any transformation.
This is useful in problems where the output is continuous, for example, predicting house prices.

Although all of the previous functions are useful for some sitautions, 
they fail to find a pattern if the data is non-linearly separable
since the function will only be able to draw a linear decesion boundary that can devide the data into two groups.
Therefore, other functions were used to find non-linear seprations between the data such as sigmoid, ReLU and tanh/ Hyperbolic Tangent. 

\begin{figure}
    \includegraphics[height=4cm]{linear}
    \caption{Example of a linear activation function on non linearly separable data
    https://www.r-bloggers.com/interactive-visualization-of-non-linear-logistic-regression-decision-boundaries-with-shiny/}
\end{figure}

\begin{table}
    \caption{diffrent Activation functions.}\label{tab1}
    \begin{tabular}{|l|l|l|}
    \hline
    Function name &  Formule & Graph\\
    \hline
    Sigmoid &  {$ h_ \theta (x) =  \frac{\mathrm{1} }{\mathrm{1} + e^{x} }  $ } &  \begin{minipage}{.3\textwidth}
        \includegraphics[width=\linewidth, height=30mm]{sigmoid}
      \end{minipage} \\
    ReLU &  {$Relu(x) = max(0,x) $} & \begin{minipage}{.3\textwidth}
        \includegraphics[width=\linewidth, height=30mm]{relu}
      \end{minipage}\\
    tanh/ Hyperbolic Tangent & {$\tanh({x})$} & \begin{minipage}{.3\textwidth}
        \includegraphics[width=\linewidth, height=30mm]{tanh}
      \end{minipage}\\
    \hline
    \end{tabular}
\end{table}

After calculating the activation unit for the inputs, a loss function $L$ is calculated for each prediction to measure 
diffrent it is from the real data. the formula of the loss function varies based on the type of the labels.
For example, if the labels are discrete valus or categories, 
Cross-Entropy could be used. Cross-Entropy increases when the predicted probability diverges from the actaul label. 





% \begin{figure}
% \includegraphics[height=6cm]{neuron}
% \caption{https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/}
% \end{figure}


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}
\bibitem{ref_article1}
Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\bibitem{ref_lncs1}
Author, F., Author, S.: Title of a proceedings paper. In: Editor,
F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
Springer, Heidelberg (2016). \doi{10.10007/1234567890}

\bibitem{ref_book1}
Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
Location (1999)

\bibitem{ref_proc1}
Author, A.-B.: Contribution title. In: 9th International Proceedings
on Proceedings, pp. 1--2. Publisher, Location (2010)

\bibitem{ref_url1}
LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
Oct 2017
\end{thebibliography}
\end{document}
