\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{hassoun1995fundamentals}
\citation{frankpic}
\citation{amari1982competition}
\citation{hochreiter1997long}
\citation{lecun1998gradient}
\HyPL@Entry{0<</S/D>>}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\@writefile{toc}{\contentsline {title}{Neural Networks and Deep Learning }{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Mohamed Hesham Ibrahim Abdalla \and Slim Abdennadher \and Alia Elbolock}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\newlabel{fpp}{{1}{1}{Introduction}{section.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Frank Rosenblatt with a Mark I Perceptron computer \cite  {frankpic}.}}{1}{figure.1.1}\protected@file@percent }
\citation{nnimage}
\citation{nnimage}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Background and Concept}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Forward Propagation}{2}{subsection.1.2.1}\protected@file@percent }
\newlabel{fb}{{2.1}{2}{Forward Propagation}{subsection.1.2.1}{}}
\newlabel{nn}{{2.1}{2}{Forward Propagation}{subsection.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces (a) A single-layer \gls {ann} where the first layer has only one neuron. (b) A multi-layer \gls {ann} with 2 hidden layers \cite  {nnimage}. }}{2}{figure.1.2}\protected@file@percent }
\newlabel{z}{{1}{3}{Forward Propagation}{equation.1.2.1}{}}
\newlabel{exsep}{{2.1}{3}{Forward Propagation}{equation.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A Figure that shows a sepration line with $n_{x}$ = 1, $W$ = 1, $b$ = -4 ($1x-4$).}}{3}{figure.1.3}\protected@file@percent }
\newlabel{a}{{2}{3}{Forward Propagation}{equation.1.2.2}{}}
\newlabel{zeq1}{{3}{3}{Forward Propagation}{equation.1.2.3}{}}
\newlabel{zeq2}{{4}{3}{Forward Propagation}{equation.1.2.4}{}}
\citation{nonsep}
\newlabel{b}{{5}{4}{Forward Propagation}{equation.1.2.5}{}}
\newlabel{nls}{{2.1}{4}{Forward Propagation}{equation.1.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of a linear activation function on non-linearly separable data \cite  {nonsep}. }}{4}{figure.1.4}\protected@file@percent }
\newlabel{log}{{6}{4}{Forward Propagation}{equation.1.2.6}{}}
\citation{lemarechal2012cauchy}
\citation{gdwiki}
\citation{gdwiki}
\newlabel{maeq}{{7}{5}{Forward Propagation}{equation.1.2.7}{}}
\newlabel{mseq}{{8}{5}{Forward Propagation}{equation.1.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Back Propagation}{5}{subsection.1.2.2}\protected@file@percent }
\newlabel{gd}{{2.2}{5}{Back Propagation}{subsection.1.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Gradient descent visualization where the function is moving from a local maxima to a local minima \cite  {gdwiki}.}}{5}{figure.1.5}\protected@file@percent }
\newlabel{gdu}{{9}{5}{Back Propagation}{equation.1.2.9}{}}
\newlabel{gdu}{{10}{5}{Back Propagation}{equation.1.2.10}{}}
\citation{sdc}
\citation{sdc}
\citation{hoffmann1992computer}
\citation{lecun1998gradient}
\newlabel{actfig}{{2.2}{6}{Back Propagation}{equation.1.2.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A figure of some activation functions where the first one is sigmoid, the second is Tanh and the last is ReLU \cite  {sdc}.}}{6}{figure.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Applications and Architectures}{6}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Convolutional Neural Networks}{6}{subsection.1.3.1}\protected@file@percent }
\citation{appf}
\citation{maxpool}
\newlabel{fm}{{3.1}{7}{Convolutional Neural Networks}{subsection.1.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A filter of size 3x3 applied on a 5x5 binary image \cite  {appf}.}}{7}{figure.1.7}\protected@file@percent }
\newlabel{mp}{{3.1}{7}{Convolutional Neural Networks}{figure.1.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces a 2x2 Max Pooling operator on a 4x4 gray scale image \cite  {maxpool}.}}{7}{figure.1.8}\protected@file@percent }
\citation{cnnpro}
\citation{cnnpro}
\citation{mckinney2020international}
\citation{mitlecthree}
\citation{hochreiter1997long}
\newlabel{dcnnp}{{3.1}{8}{Convolutional Neural Networks}{figure.1.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A CNN applied on a boat image to detect if it falls in dog, cat, boat or bird categories \cite  {cnnpro}.}}{8}{figure.1.9}\protected@file@percent }
\newlabel{can}{{3.1}{8}{Convolutional Neural Networks}{figure.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A figure showing that AI and \gls {cnns} can outperform RD Readers \cite  {mitlecthree}.}}{8}{figure.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Recurrent Neural Networks}{8}{subsection.1.3.2}\protected@file@percent }
\citation{werbos1990backpropagation}
\citation{mitlectwo}
\citation{mitlectwo}
\citation{jagannatha2016structured}
\citation{gu2020lstm}
\citation{earthnet}
\citation{hou2017deep}
\citation{yan2016attribute2image}
\citation{berner2019dota}
\citation{openairubikcube}
\newlabel{rnn}{{11}{9}{Recurrent Neural Networks}{equation.1.3.11}{}}
\newlabel{rnny}{{12}{9}{Recurrent Neural Networks}{equation.1.3.12}{}}
\newlabel{rnnf}{{3.2}{9}{Recurrent Neural Networks}{equation.1.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces An unfolded RNN where $W_{hy}$ is the weight vector between the state and the output , $W_{xh}$ is the weight vector between the current input $x_{t}$ and the state $h_{t}$ \cite  {mitlectwo}.}}{9}{figure.1.11}\protected@file@percent }
\bibstyle{splncs04}
\bibdata{refs}
\bibcite{earthnet}{1}
\bibcite{appf}{2}
\bibcite{maxpool}{3}
\bibcite{frankpic}{4}
\bibcite{sdc}{5}
\bibcite{gdwiki}{6}
\bibcite{nonsep}{7}
\bibcite{cnnpro}{8}
\bibcite{mitlectwo}{9}
\bibcite{mitlecthree}{10}
\bibcite{openairubikcube}{11}
\bibcite{nnimage}{12}
\bibcite{amari1982competition}{13}
\bibcite{berner2019dota}{14}
\bibcite{gu2020lstm}{15}
\bibcite{hassoun1995fundamentals}{16}
\bibcite{hochreiter1997long}{17}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{10}{section.1.4}\protected@file@percent }
\bibcite{hoffmann1992computer}{18}
\bibcite{hou2017deep}{19}
\bibcite{jagannatha2016structured}{20}
\bibcite{lecun1998gradient}{21}
\bibcite{lemarechal2012cauchy}{22}
\bibcite{mckinney2020international}{23}
\bibcite{werbos1990backpropagation}{24}
\bibcite{yan2016attribute2image}{25}
